{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:陳昇\n",
    "\n",
    "Student ID:108062140\n",
    "\n",
    "GitHub ID:sam1491591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2022-Lab1-Master](https://github.com/keziatamus/DM2022-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2022-Lab1-Master](https://github.com/keziatamus/DM2022-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/keziatamus/DM2022-Lab1-Homework/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 20th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "\n",
    "# TEST necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# obtain the documents containing the categories provided\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** >>> Exercise 1 (5 min): **\n",
    "In this exercise, please print out the text data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n",
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n",
      "From: djohnson@cs.ucsd.edu (Darin Johnson)\n",
      "Subject: Re: harrassed at work, could use some prayers\n",
      "Organization: =CSE Dept., U.C. San Diego\n",
      "Lines: 63\n",
      "\n",
      "(Well, I'll email also, but this may apply to other people, so\n",
      "I'll post also.)\n",
      "\n",
      ">I've been working at this company for eight years in various\n",
      ">engineering jobs.  I'm female.  Yesterday I counted and realized that\n",
      ">on seven different occasions I've been sexually harrassed at this\n",
      ">company.\n",
      "\n",
      ">I dreaded coming back to work today.  What if my boss comes in to ask\n",
      ">me some kind of question...\n",
      "\n",
      "Your boss should be the person bring these problems to.  If he/she\n",
      "does not seem to take any action, keep going up higher and higher.\n",
      "Sexual harrassment does not need to be tolerated, and it can be an\n",
      "enormous emotional support to discuss this with someone and know that\n",
      "they are trying to do something about it.  If you feel you can not\n",
      "discuss this with your boss, perhaps your company has a personnel\n",
      "department that can work for you while preserving your privacy.  Most\n",
      "companies will want to deal with this problem because constant anxiety\n",
      "does seriously affect how effectively employees do their jobs.\n",
      "\n",
      "It is unclear from your letter if you have done this or not.  It is\n",
      "not inconceivable that management remains ignorant of employee\n",
      "problems/strife even after eight years (it's a miracle if they do\n",
      "notice).  Perhaps your manager did not bring to the attention of\n",
      "higher ups?  If the company indeed does seem to want to ignore the\n",
      "entire problem, there may be a state agency willing to fight with\n",
      "you.  (check with a lawyer, a women's resource center, etc to find out)\n",
      "\n",
      "You may also want to discuss this with your paster, priest, husband,\n",
      "etc.  That is, someone you know will not be judgemental and that is\n",
      "supportive, comforting, etc.  This will bring a lot of healing.\n",
      "\n",
      ">So I returned at 11:25, only to find that ever single\n",
      ">person had already left for lunch.  They left at 11:15 or so.  No one\n",
      ">could be bothered to call me at the other building, even though my\n",
      ">number was posted.\n",
      "\n",
      "This happens to a lot of people.  Honest.  I believe it may seem\n",
      "to be due to gross insensitivity because of the feelings you are\n",
      "going through.  People in offices tend to be more insensitive while\n",
      "working than they normally are (maybe it's the hustle or stress or...)\n",
      "I've had this happen to me a lot, often because they didn't realize\n",
      "my car was broken, etc.  Then they will come back and wonder why I\n",
      "didn't want to go (this would tend to make me stop being angry at\n",
      "being ignored and make me laugh).  Once, we went off without our\n",
      "boss, who was paying for the lunch :-)\n",
      "\n",
      ">For this\n",
      ">reason I hope good Mr. Moderator allows me this latest indulgence.\n",
      "\n",
      "Well, if you can't turn to the computer for support, what would\n",
      "we do?  (signs of the computer age :-)\n",
      "\n",
      "In closing, please don't let the hateful actions of a single person\n",
      "harm you.  They are doing it because they are still the playground\n",
      "bully and enjoy seeing the hurt they cause.  And you should not\n",
      "accept the opinions of an imbecile that you are worthless - much\n",
      "wiser people hold you in great esteem.\n",
      "-- \n",
      "Darin Johnson\n",
      "djohnson@ucsd.edu\n",
      "  - Luxury!  In MY day, we had to make do with 5 bytes of swap...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))\n",
    "print(\"\\n\".join(twenty_train.data[1].split(\"\\n\")))\n",
    "print(\"\\n\".join(twenty_train.data[2].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers.data_mining_helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-250bad10d019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# my functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_mining_helpers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdmh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# construct dataframe from a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helpers.data_mining_helpers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# construct dataframe from a list\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
